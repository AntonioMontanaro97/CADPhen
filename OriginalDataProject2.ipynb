{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd973cc",
   "metadata": {},
   "source": [
    "Nello script \"OriginalDataProject\" è stato scelto il miglior algortimo di imputazione. A questo punto estraggo dal dataset principale estreggo circa 800 items per cui ho la classificazione in \"INTESTRAT-CAD_TC_class.xlsx\". Sarà su questo database che effettuero l'imputing di tutte le colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import statsmodels as stm\n",
    "import missingno as msno\n",
    "import statsmodels.imputation.mice as mice\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import random \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import os \n",
    "\n",
    "#Kepler Mayer\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "import kmapper as km\n",
    "from sklearn import preprocessing\n",
    "from kmapper.plotlyviz import *\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import (HBox, VBox)\n",
    "import sys\n",
    "import operator\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gower as gw \n",
    "\n",
    "#Per prima cosa verifichiamo la normalità dei dati\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "import pathlib\n",
    "import scipy.stats as scy\n",
    "\n",
    "# To use the IterativeImputer, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import (HBox, VBox)\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "#Funzioni per la NetworkX \n",
    "from networkx.algorithms import community\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import networkx.algorithms.community as nxcom\n",
    "\n",
    "\n",
    "from pySankey.sankey import sankey\n",
    "\n",
    "rnd = np.random.RandomState(17489)\n",
    "st = random.getstate()  # remeber this state \n",
    "\n",
    "# Definisco una color-map che associa un colore ai diversi valori in base al bin di appartenenza\n",
    "pl_brewer = [[0.0, '#fcefb4'],  \n",
    "             [0.1, '#FFBA08'],\n",
    "             [0.2, '#FAA307'],\n",
    "             [0.3, '#F48C06'],\n",
    "             [0.4, '#E85D04'],\n",
    "             [0.5, '#DC2F02'],\n",
    "             [0.6, '#D00000'],\n",
    "             [0.7, '#9D0208'],\n",
    "             [0.8, '#6A040F'],\n",
    "             [0.9, '#370617'],\n",
    "             [1.0, '#03071E']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzioni\n",
    "def read_csv(path):\n",
    "    \"\"\"\n",
    "    Funzione necessaria a leggere un file csv senza che \"dtype\" delle diverse variabili\n",
    "    venga perso.\n",
    "    INPUT:\n",
    "    - path: percorso e nome del file da salvare\n",
    "    OUTPUT:\n",
    "    salvataggio del file\n",
    "    \"\"\"\n",
    "    # Leggo dalla prima riga il tipo delle rispettive variabili\n",
    "    dtypes = pd.read_csv(path, nrows=1).iloc[0].to_dict()\n",
    "    # Leggo il resto delle righe e quindi i dati\n",
    "    return pd.read_csv(path, dtype=dtypes, skiprows=[1])\n",
    "\n",
    "\n",
    "def to_csv(df, path):\n",
    "    \"\"\"\n",
    "    Funzione necessaria per poter conservare il \"dtype\" anche nella fase di salvataggio e \n",
    "    successiva rilettura.\n",
    "    INPUT:\n",
    "    - df: dataframe da salvare\n",
    "    - path: percorso e nome del file da salvare\n",
    "    \"\"\"\n",
    "    # Salvo il tipo delle variabili salvandolo come nuova riga nel file\n",
    "    df.loc[-1] = df.dtypes\n",
    "    df.index = df.index + 1\n",
    "    df.sort_index(inplace=True)\n",
    "    # Successivamente salvo il file csv\n",
    "    df.to_csv(path, index=False)\n",
    "    \n",
    "def find_file(path,df,flag):\n",
    "    \"\"\"\n",
    "    Funzione che permette di verificare se un file è presente o meno\n",
    "    INPUT:\n",
    "    - path: percorso del file con nome finale del file\n",
    "    - df: dataframe che si vuole salvare\n",
    "    - flag: segnale che mi permette di salvare e quindi sovrascrivere il file anche se presente\n",
    "    \"\"\"\n",
    "    if not(flag):\n",
    "        if os.path.isfile(path):\n",
    "            print(\"Il file esiste! Se vuoi salvare comunque il file cambia il flag a 1\")\n",
    "        else:\n",
    "            print(\"Il file non esiste! Verrà salvato nella cartella File Memory\")\n",
    "            to_csv(df,path)\n",
    "            \n",
    "    else:\n",
    "        to_csv(df,path)\n",
    "\n",
    "\n",
    "def entropy_count(scomplex):\n",
    "    \"\"\"\n",
    "    Funzione che mi permette di calcolare l'entropia del grafo.\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale\n",
    "    OUTPUT:\n",
    "    - entropy_graph: entropia media del grafo\n",
    "    \"\"\"\n",
    "    kmgraph = get_mapper_graph(scomplex)\n",
    "    # assign to node['custom_tooltips']  the node label (0 for absence of  cardiac insufficiency, 1 for different type of cardiac insufficiency)\n",
    "    for node in kmgraph[0]['nodes']:\n",
    "        node['custom_tooltips'] = y_class[scomplex['nodes'][node['name']]]\n",
    "        Entropy_node ={}\n",
    "    for j, node in enumerate(kmgraph[0]['nodes']):   #j è l'id del nodo mentre in node vi sono le caratteristiche del nodo\n",
    "        dimCluster = node['cluster']['size']\n",
    "        data = node['custom_tooltips']\n",
    "        dimData0 = len(data.loc[data==0])\n",
    "        dimData1 = len(data.loc[data==1])\n",
    "        if dimData0 != 0 and dimData1 !=0:\n",
    "            H_node = -((dimData0/dimCluster)*math.log2(dimData0/dimCluster)+(dimData1/dimCluster)*math.log2(dimData1/dimCluster))\n",
    "        if dimData0 != 0 and dimData1 ==0:\n",
    "            H_node = -((dimData0/dimCluster)*math.log2(dimData0/dimCluster))\n",
    "        if dimData0 == 0 and dimData1 !=0:\n",
    "            H_node = -((dimData1/dimCluster)*math.log2(dimData1/dimCluster))\n",
    "                \n",
    "        Entropy_node[j] = [dimCluster, H_node]\n",
    "    sumEntropies = 0\n",
    "    numberData = 0\n",
    "    for j, node in enumerate(Entropy_node):\n",
    "        sumEntropies = sumEntropies + (Entropy_node[j][0]*Entropy_node[j][1])\n",
    "        numberData = numberData + Entropy_node[j][0]\n",
    "    entropy_graph = sumEntropies/numberData\n",
    "    return entropy_graph\n",
    "\n",
    "\n",
    "\n",
    "def do_graph(scomplex, title, y_class, my_colorscale):\n",
    "    \"\"\"\n",
    "    Funzione che genera il grafico tramite plotly\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale\n",
    "    - title: titolo del grafico\n",
    "    - y_class: vettore contenente la classificazione \n",
    "    my_colorscale: color map utilizzata per colorare il grafo\n",
    "    OUTPUT:\n",
    "    - fwn: figure widget contenente il grafico\n",
    "    \"\"\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min',\n",
    "                                                                 colorscale=my_colorscale)\n",
    "    \n",
    "    \n",
    "    # assegno al node['custom_tooltips'] il label (0 per CAD = 0, 1 per CAD = 1,2)\n",
    "    for node in kmgraph['nodes']:\n",
    "        node['custom_tooltips'] = y_class[scomplex['nodes'][node['name']]]\n",
    "    \n",
    "    bgcolor = '#e9ecef'    #è il colore dello sfondo su cui vi è la rete\n",
    "    y_gridcolor = 'rgb(150,150,150)'# è il colore della griglia\n",
    "\n",
    "    plotly_graph_data = plotly_graph(kmgraph, graph_layout='fr', colorscale=my_colorscale,\n",
    "                                 factor_size=4.0, edge_linewidth=1.0)\n",
    "    layout = plot_layout(title='Topological network representing the<br>  cardiac insufficiency',\n",
    "                     width=700, height=750,\n",
    "                     annotation_text=title + \"<br>\" + get_kmgraph_meta(mapper_summary), \n",
    "                     bgcolor=bgcolor)\n",
    "\n",
    "    fw_graph = go.FigureWidget(data=plotly_graph_data, layout=layout)\n",
    "\n",
    "\n",
    "    heartFail_dict = {0: 'Class_0', 1: 'Class_1'}\n",
    "    tooltips = list(fw_graph.data[1].text) # eseguo questo cast perchè fw.data[1].text è una tupla che vogliamo aggiungere al\n",
    "                                     # tooltips\n",
    "\n",
    "    new_color = []  #è un vettore tanto lungo quanti sono i nodi e con numeri tra 0 e 1\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        member_label_ids = y_class[scomplex['nodes'][node['name']]]         #al node j-esimo associo 0-1 in base alla classe  (se volessi colorare rispetto ad altro basta modificare questa riga e il rispettivo dict)\n",
    "        member_labels = [heartFail_dict[id] for id in member_label_ids]     #in questo modo ad opgni nodo viene creato un vettore contentente class0 o class1 in base ai pazienti che apprtengono a quel nodo\n",
    "        label_type, label_counts = np.unique(member_labels, return_counts=True)  #in questo modo so la frequenza della classi nel nodo e in ogni nodo\n",
    "\n",
    "        n_members = label_counts.sum()\n",
    "        if label_type.shape[0] == 1:          # Se presente solo la classe 1 o classe 0 vuol dire che il vettore ha solo una dimensione \n",
    "            if label_type[0] == 'Class_0':\n",
    "                new_color.append(0.0)           # Associo il colore basso del range\n",
    "            else:\n",
    "                new_color.append(1.0)           # Associo il colore alto del range\n",
    "        else:\n",
    "            new_color.append(1.0*label_counts[1]/n_members)   # Associo il colore in base alla proporzione della Classe 0\n",
    "\n",
    "        for m in range(len(label_counts)):    # Associo ad ogni nodo \"cluster\" il numero di elementi \n",
    "            tooltips[j] += '<br>' + str(label_type[m]) + ': ' + str(label_counts[m]) # append  how many different\n",
    "\n",
    "    \n",
    "    \n",
    "    fwn_graph = go.FigureWidget(fw_graph) # recupero la figura creata precedentemente\n",
    "    with fwn_graph.batch_update(): # eseguo l'update dalla nuova figure\n",
    "        fwn_graph.data[1].text = tooltips # aggiungo i nuovi tooltips\n",
    "        fwn_graph.data[1].marker.colorbar.x = -0.14 # definisco la posizione della toolbar\n",
    "        fwn_graph.layout.width = 550 # definisco le dimensioni della finestra per ottenere due copie parallele\n",
    "        fwn_graph.layout.height = 550\n",
    "        fwn_graph.layout.margin.r = 20 # diminuisco il margine destro da 60px (default val) a 45 pixels\n",
    "        fwn_graph.data[1].marker.color = new_color # aggiorno i colori dei nodi\n",
    "\n",
    "    return fwn_graph\n",
    "\n",
    "        \n",
    "def do_enrinchment(scomplex,my_colorscale, var_cont, var_cont_norm, fwn,  title):\n",
    "    \"\"\"\n",
    "    Funzione che permette di colorare la rete ricavata dalla TDA rispetto ad una determinata\n",
    "    variabile (continua) che caratterizza i pazienti appartenenti ai nodi\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale ottenuto dal package TDA Mapper\n",
    "    - my_colorscale: color map scelta\n",
    "    - var_cont: variabile continua\n",
    "    - var-cont_norm: variabile continua normalizzata\n",
    "    - fwn: figure widget contenente la rete colorata rispetto alla classe CAD\n",
    "    - title: titolo da inserire nel grafico\n",
    "    OUTPUT:\n",
    "    - fwn: figure widget contenente il grafico\n",
    "    \"\"\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min',\n",
    "                                                                 colorscale=my_colorscale)\n",
    "    \n",
    "    tooltipsVar = list(fwn.data[1].text) # eseguo questo cast perchè fw.data[1].text è una tupla che vogliamo aggiungere al\n",
    "                                     # tooltips       \n",
    "    heartFail_dict = {0: 'Class_0', 1: 'Class_1'}\n",
    "    new_color_var = []  #è un vettore tanto lungo quanti sono i nodi e con numeri tra 0 e 1\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        member_label_ids = y_class[scomplex['nodes'][node['name']]], var_cont[scomplex['nodes'][node['name']]], var_cont_norm[scomplex['nodes'][node['name']]]          #al node j-esimo associo 0-1 in base alla classe e all'età (se volessi colorare rispetto ad altro basta modificare questa riga e il rispettivo dict)\n",
    "        member_labels = [heartFail_dict[id] for id in member_label_ids[0]]     #in questo modo ad ogni nodo viene creato un vettore contentente class0 o class1 in base ai pazienti che appartengono a quel nodo\n",
    "        label_type, label_counts = np.unique(member_labels, return_counts=True)  #in questo modo so la frequenza della classi nel nodo e in ogni nodo\n",
    "        mean_var_members = member_label_ids[1].mean()\n",
    "        n_members = label_counts.sum()\n",
    "    \n",
    "        #Associo il colore direttamente in base all'età media che essendo scalata avrà valore tra 0(media bassa) e 1\n",
    "        new_color_var.append(member_label_ids[2].mean())\n",
    "        tooltipsVar[j] += '<br>' + \"Mean:\" + str(mean_var_members)  #In questo modo visualizzero l'età media del nodo (non rispetto alla classe)\n",
    "           \n",
    "    fwVar = go.FigureWidget(fwn) #definisco la nuova Figure da fwn_graph che verrà colorato in modo diverso\n",
    "    with fwVar.batch_update():\n",
    "        fwVar.data[1].marker.color = new_color_var # carico i nuovi colori\n",
    "        fwVar.data[0].line.color = 'rgb(125,125,125)' # carico il nuovo colore degli edges\n",
    "        fwVar.data[1].text = tooltipsVar # aggiungo la nuova tooltips\n",
    "        fwVar.layout.plot_bgcolor = '#edf2ef'\n",
    "        fwVar.layout.annotations = None # rimuovo il mapper_summary dal secondo plot\n",
    "        fwVar.data[1].marker.showscale = False # rimuovo la color bar\n",
    "        fwVar.layout.title = \"Nodes are colored according to the average<br>\"+ \"members's\" + title \n",
    "    return fwVar\n",
    "\n",
    "\n",
    "def do_enrinchment_cat(scomplex,my_colorscale, var_cat, fwn,  title, cat_dict):\n",
    "    \"\"\"\n",
    "    Funzione che permette di colorare la rete rispetto ad una variabile categorica a due livelli\n",
    "    che caratterizza i pazienti appartenenti ai singoli nodi.\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale ottenuto dal package TDA Mapper\n",
    "    - my_colorscale: color map scelta\n",
    "    - var_cat: variabile continua\n",
    "    - title: titolo del grafico \n",
    "    - fwn: figure widget contenente la rete colorata rispetto alla classe CAD\n",
    "    - cat_dict: dizionario contenente il label della variabile categorica\n",
    "    OUTPUT:\n",
    "    - fwn: figure widget contenente il grafico\n",
    "    \"\"\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min',\n",
    "                                                                 colorscale=my_colorscale)\n",
    "    \n",
    "    tooltipsVar = list(fwn.data[1].text)# eseguo questo cast perchè fw.data[1].text è una tupla che vogliamo aggiungere al\n",
    "                                     # tooltips\n",
    "    new_color = []  #è un vettore tanto lungo quanti sono i nodi e con numeri tra 0 e 1\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        member_label_ids = y_class[scomplex['nodes'][node['name']]], var_cat[scomplex['nodes'][node['name']]]          #al node j-esimo associo 0-1 in base alla classe e all'età (se volessi colorare rispetto ad altro basta modificare questa riga e il rispettivo dict)\n",
    "        member_labels = [cat_dict[id] for id in member_label_ids[1]]     #in questo modo ad ogni nodo viene creato un vettore contentente class0 o class1 in base ai pazienti che appartengono a quel nodo\n",
    "        label_type, label_counts = np.unique(member_labels, return_counts=True)  #in questo modo so la frequenza della classi nel nodo e in ogni nodo\n",
    "        \n",
    "        n_members = label_counts.sum()\n",
    "        if label_type.shape[0] == 1:          # Se presente solo la classe 1 o classe 0 vuol dire che il vettore ha solo una dimensione \n",
    "            if label_type[0] == 'Cat_0':\n",
    "                new_color.append(0.0)           # Associo il colore basso del range\n",
    "            else:\n",
    "                new_color.append(1.0)           # Associo il colore alto del range\n",
    "        else:\n",
    "            new_color.append(1.0*label_counts[1]/n_members)   # Associo il colore in base alla proporzione della Classe 0\n",
    "\n",
    "        for m in range(len(label_counts)):    # Associo ad ogni nodo \"cluster\" il numero di elementi \n",
    "            tooltipsVar[j] += '<br>' + str(label_type[m]) + ': ' + str(label_counts[m]) # appendo le quantità dei diversi label    \n",
    "    \n",
    "    fwVar = go.FigureWidget(fwn) #definisco la nuova Figure da fwn_graph che verrà colorato in modo diverso\n",
    "    with fwVar.batch_update():\n",
    "        fwVar.data[1].marker.color = new_color # aggiorno il colore dei nodi\n",
    "        fwVar.data[0].line.color = 'rgb(125,125,125)' # aggiorno il colore degli edges\n",
    "        fwVar.data[1].text = tooltipsVar # aggiungo i nuovi tooltips\n",
    "        fwVar.layout.plot_bgcolor = '#edf2ef'\n",
    "        fwVar.layout.annotations = None # rimuovo il mapper_summary dal secondo plot\n",
    "        fwVar.data[1].marker.showscale = False #  rimuovo la colorbar\n",
    "        fwVar.layout.title = \"Nodes are colored according to the average<br>\"+ \"members's\" + title \n",
    "    return fwVar\n",
    "\n",
    "\n",
    "def do_hist_counter(scomplex,title,path):\n",
    "    \"\"\"\n",
    "    Funzione che permette di calcolare la distribuzione della numerosità dei diversi nodi che\n",
    "    caratterizzano la rete.\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale ottenuto dal package TDA Mapper\n",
    "    - title: titolo del grafico \n",
    "    - path: percorso e nome del file da salvare\n",
    "    OUTPUT:\n",
    "    - bar_dist: \n",
    "    \"\"\"    \n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min')\n",
    "    \n",
    "    counter = list()\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        counter.append(node[\"cluster\"][\"size\"])\n",
    "    counter_dist = Counter(counter)\n",
    "    counter_dist = OrderedDict(sorted(counter_dist.items(), key=lambda kv: kv[0]))\n",
    "    \n",
    "    #Creo l'immagine \n",
    "    chiavi = list(counter_dist.keys())\n",
    "    valori = list(counter_dist.values())\n",
    "    \n",
    "    ax  = plt.figure(figsize=(10,7))  \n",
    "    barDist = sns.barplot(x=chiavi, y= valori, palette=\"Blues_d\")\n",
    "    plt.suptitle(title)\n",
    "    plt.savefig(path,width = 300, height =300)\n",
    "\n",
    "    return barDist\n",
    "\n",
    "\n",
    "#Funzioni per la gestione del NetworkX\n",
    "def set_node_community(G, communities):\n",
    "        \"\"\"\n",
    "        Funzione che aggiunge alla community  il nodo come attributo.\n",
    "        INPUT:\n",
    "        - G: grafo    \n",
    "        - communities: comminities trovate\n",
    "        \"\"\"\n",
    "        for c, v_c in enumerate(communities):\n",
    "            for v in v_c:\n",
    "                # Aggiungo 1 per salvare l'edge esterno\n",
    "                G.nodes[v]['community'] = c + 1\n",
    "\n",
    "def set_edge_community(G):\n",
    "        \"\"\"\n",
    "        Funzione che cerca gli edges interni alla communities e li aggiunge.\n",
    "        INPUT:\n",
    "        - G: grafo        \n",
    "        \"\"\"\n",
    "        for v, w, in G.edges:\n",
    "            if G.nodes[v]['community'] == G.nodes[w]['community']:\n",
    "                # I lati interni vengono marcati nella community\n",
    "                G.edges[v, w]['community'] = G.nodes[v]['community']\n",
    "            else:\n",
    "                # I lati esterni vengono marcati con 0\n",
    "                G.edges[v, w]['community'] = 0\n",
    "\n",
    "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
    "        \"\"\"\n",
    "        Funzione che assegna un colore ai vertici della stessa communities.\n",
    "        INPUT:\n",
    "        i: contatore \n",
    "        \"\"\"\n",
    "        \n",
    "        r0, g0, b0 = 0, 0, 0\n",
    "        n = 16\n",
    "        low, high = 0.1, 0.9\n",
    "        span = high - low\n",
    "        r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
    "        g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
    "        b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
    "        return (r, g, b)\n",
    "    \n",
    "def do_color(scomplex, var_cont, var_cont_norm, dict_class,my_colorscale):\n",
    "    \"\"\"\n",
    "    Funzione che permette di colorare la rete rispetto ad una variabile continua\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale ottenuto dal package TDA Mapper\n",
    "    - var_cont: variabile continua\n",
    "    - var_cont_norm: variabile continua normalizzata\n",
    "    - dict_class: dizionario contenente i labels della classe\n",
    "    - my_colorscale: color map scelta\n",
    "    OUTPUT:\n",
    "    - new_color_var: vettore contenete il colore di ogni nodo \n",
    "    \"\"\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min',\n",
    "                                                                 colorscale=my_colorscale)\n",
    "\n",
    "    new_color_var= []  #è un vettore tanto lungo quanti sono i nodi e con numeri tra 0 e 1\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        member_label_ids = y_class[scomplex['nodes'][node['name']]], var_cont[scomplex['nodes'][node['name']]], var_cont_norm[scomplex['nodes'][node['name']]]          #al node j-esimo associo 0-1 in base alla classe e all'età (se volessi colorare rispetto ad altro basta modificare questa riga e il rispettivo dict)\n",
    "        member_labels = [dict_class[id] for id in member_label_ids[0]]     #in questo modo ad ogni nodo viene creato un vettore contentente class0 o class1 in base ai pazienti che appartengono a quel nodo\n",
    "        label_type, label_counts = np.unique(member_labels, return_counts=True)  #in questo modo so la frequenza della classi nel nodo e in ogni nodo\n",
    "        mean_var_members = member_label_ids[1].mean()\n",
    "        n_members = label_counts.sum()\n",
    "    \n",
    "        #Associo il colore direttamente in base all'età media che essendo scalata avrà valore tra 0(media bassa) e 1\n",
    "        new_color_var.append(member_label_ids[2].mean())\n",
    "\n",
    "    return new_color_var \n",
    "\n",
    "def do_color_class(scomplex,y_class,dict_class,my_colorscale):\n",
    "    \"\"\"\n",
    "    Funzione che permette di colorare la rete rispetto alla classe CAD\n",
    "    INPUT:\n",
    "    - scomplex: complesso simpliciale ottenuto dal package TDA Mapper\n",
    "    - y_class: vettore classificazione\n",
    "    - dict_class: dizionario contenente i labels della classe\n",
    "    - my_colorscale: color map scelta\n",
    "    OUTPUT:\n",
    "    - new_color_class: vettore contenete il colore di ogni nodo \n",
    "    \"\"\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,\n",
    "                                                                 color_function_name='Distance to x-min',\n",
    "                                                                 colorscale=my_colorscale)\n",
    "    \n",
    "    new_color_class = []  #è un vettore tanto lungo quanti sono i nodi e con numeri tra 0 e 1\n",
    "    for j, node in enumerate(kmgraph['nodes']):\n",
    "        member_label_ids = y_class[scomplex['nodes'][node['name']]]         #al node j-esimo associo 0-1 in base alla classe  (se volessi colorare rispetto ad altro basta modificare questa riga e il rispettivo dict)\n",
    "        member_labels = [dict_class[id] for id in member_label_ids]     #in questo modo ad opgni nodo viene creato un vettore contentente class0 o class1 in base ai pazienti che apprtengono a quel nodo\n",
    "        label_type, label_counts = np.unique(member_labels, return_counts=True)  #in questo modo so la frequenza della classi nel nodo e in ogni nodo\n",
    "\n",
    "        n_members = label_counts.sum()\n",
    "        if label_type.shape[0] == 1:          # Se presente solo la classe 1 o classe 0 vuol dire che il vettore ha solo una dimensione \n",
    "            if label_type[0] == 'Class_0':\n",
    "                new_color_class.append(0.0)           # Associo il colore basso del range\n",
    "            else:\n",
    "                new_color_class.append(1.0)           # Associo il colore alto del range\n",
    "        else:\n",
    "            new_color_class.append(1.0*label_counts[1]/n_members)   # Associo il colore in base alla proporzione della Classe 0\n",
    "    return new_color_class\n",
    "\n",
    "#Con questa funzione va a cercare le community all'interno della rete\n",
    "def community_searching(G):\n",
    "    \"\"\"\n",
    "    Funzione che permette di applicare l'algoritmo di ricerca \n",
    "    delle community all'interno della rete\n",
    "    INPUT:\n",
    "    - G: grafo\n",
    "    OUTPUT:\n",
    "    - node_color: colore dei nodi \n",
    "    - internal: lista dei nodi appartenenti alle communities\n",
    "    - internal_color: colore delle diverse communities\n",
    "    - external: nodi esterni\n",
    "    \"\"\"\n",
    "    #Estraggo le communieties all'interno del grafo tramite metodo 'Greedy_modularity'\n",
    "    communities =girvan_newman(G)\n",
    "    # Cerco le communities a partire dalla più grande\n",
    "    communities = sorted(nxcom.greedy_modularity_communities(G), key=len , reverse=True)\n",
    "\n",
    "    # Definisco i nodi e gli edges della communities\n",
    "    set_node_community(G, communities)\n",
    "    set_edge_community(G)\n",
    "\n",
    "    node_color = [get_color(G.nodes[v]['community']) for v in G.nodes]\n",
    "\n",
    "    # Importo il colore degli edges tra membri della stessa communities (interni)\n",
    "    # e degli edges tra le diverse communities\n",
    "    external = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] == 0]\n",
    "    internal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] > 0]\n",
    "    internal_color = ['black' for e in internal]\n",
    "    return node_color, internal, internal_color,external\n",
    "\n",
    "\n",
    "\n",
    "def do_graph_NetworkX(G,color_class, path, node_color, internal, internal_color):\n",
    "    \"\"\"\n",
    "    Funzione che permette di ricavare due grafici a confronto partendo dal complesso simpliciale\n",
    "    INPUT:\n",
    "    - G: grafo \n",
    "    - color_class: vettore contenente il colore dei nodi rispetto alla classe\n",
    "    - path: percorso e nome del file in cui si vuole salvare l'immagine \n",
    "    - node_color: vettore contenente il colore dei nodi rispetto alle communities\n",
    "    - internal: lista dei lati delle commmunities interne\n",
    "    - internal_color: colore dei nodi appartenenti alle diverse communities\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "    # Grafico la stessa rete coni pacchetti NetworkX e igraph\n",
    "    fig, (ax0,ax1) = plt.subplots(nrows=1, ncols=2, figsize=(14, 8))\n",
    "\n",
    "    ax0.set_title(\"Plot with NetworkX - Class Proportion\")\n",
    "    nx.colormaps = pl_brewer\n",
    "    nx.draw_kamada_kawai(G, node_size=80, ax=ax0, node_color=color_class, cmap = 'coolwarm')\n",
    "    #la color map coolwarm va dal blu (0) al rosso (1) quindi nei nodi blu vi è una prevalenza di soggetti sani\n",
    "\n",
    "    ax1.set_title(\"Plot with NetworkX - Communities\")\n",
    "    nx.draw_kamada_kawai(G, node_size=50, ax=ax1, node_color= node_color, edgelist=internal, edge_color = internal_color)\n",
    "\n",
    "    plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853630c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carico il database \"INTESTRAT-CAD_TC_class.xlsx\" \n",
    "db_class = pd.read_excel(\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/INTESTRAT-CAD_TC_class.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb58bb",
   "metadata": {},
   "source": [
    "Le righe del db_class sono 829 ciò vuol dire che mi aspetto ci siano 829 pazienti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb016dca",
   "metadata": {},
   "source": [
    "In questa database troviamo la classificazione completa, classificazione molto utile poichè è assente per qualche paziente nel database generale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino la riga in cui è contenuto il paziente con \"STENT\" poichè non ha nessuna classificazione \n",
    "db_class.drop(db_class.loc[db_class[\"Class\"]=='STENT'].index, inplace = True)\n",
    "\n",
    "#Elimino le righe dei pazienti in cui non si ha una classificazione\n",
    "#Salvo l'ID dei pazienti per cui non ho classificazione\n",
    "ID_noClass = db_class.loc[db_class[\"Class\"].isna()].index   \n",
    "db_class.drop(db_class.loc[db_class[\"Class\"].isna()].index, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a8c49",
   "metadata": {},
   "source": [
    "Dopo aver cancellato la riga del paziente con Class = STENT e quelle dei pazienti senza classificazione, i pazienti diventano 826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d75c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificazione\n",
    "print(\"Pazienti con Classe 0 = \",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]))\n",
    "print(\"Pazienti con Classe 1 = \",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0]))\n",
    "print(\"Pazienti con Classe 2 = \",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0]))\n",
    "print(\"Pazienti Totali = \",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]) + len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0]) + len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = read_csv(\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/dataset_Visita1.csv\")\n",
    "del dataSet[\"Event Name\"]\n",
    "del dataSet[\"Data della visita\"]\n",
    "del dataSet[\"Classe NYHA\"]\n",
    "del dataSet[\"Se sì, specificare la Class Canadian Society\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f882",
   "metadata": {},
   "source": [
    "A questo punto vado ad estrapolare dal dataset principale \"dataset\" tutti gli items per cui vi è il matching con il codice epifania contenuto in db_class. In questo modo vado ad ottenere un dataset composto dai soli items che hanno una classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considerando la Classe CAD 0-1-2:\")\n",
    "print(\"Soggetti con Classe 0:\",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]))\n",
    "print(\"Soggetti con Classe 1:\",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0]))\n",
    "print(\"Soggetti con Classe 2:\",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0]))\n",
    "print(\"\\n\")\n",
    "print(\"Considerando la Classe CAD 0 vs 1-2:\")\n",
    "print(\"Soggetti con Classe 0:\",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]))\n",
    "print(\"Soggetti con Classe 1 e 2:\",len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0]) + len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0]))\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "class_labels = ['CAD 0', 'CAD 1', 'CAD 2']\n",
    "values = [len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]), len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0]),len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0])]\n",
    "ax.bar(class_labels,values, color = [\"#fe5f55\", \"#ffba08\",\"#648de5\"])\n",
    "plt.title(\"Classificazione CAD 0 vs 1 vs 2\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "class_labels = ['CAD 0', 'CAD 1-2']\n",
    "values = [len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 0.0]), len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 1.0])+len(db_class.loc[db_class[\"CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)\"] == 2.0])]\n",
    "ax.bar(class_labels,values, color = [\"#fe5f55\",\"#648de5\"])\n",
    "plt.title(\"Classificazione CAD 0 vs 1-2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.rename(columns = {'Codice Epifania':'Sample_ID'}, inplace = True)\n",
    "dataSet[dataSet[\"Sample_ID\"].isin(db_class[\"Sample_ID\"].tolist())]\n",
    "dataSet1 = dataSet.copy()\n",
    "db = dataSet1.merge(db_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lunghezza del dataset mergiato:\" + str(len(db)))\n",
    "print(\"lunghezza del dataset con classificazione:\" + str(len(db_class)))\n",
    "#Tra uno e l'altro mancono 3 pazienti, perchè? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95410ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleNotInClass = set(db_class[\"Sample_ID\"].tolist()) - set(dataSet[\"Sample_ID\"].tolist())\n",
    "SampleInClass = set(dataSet[\"Sample_ID\"].tolist())\n",
    "np.save(\"SampleWithoutClass\",list(SampleNotInClass))\n",
    "np.save(\"SampleClass\",list(SampleInClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719308b",
   "metadata": {},
   "source": [
    "In SampleNotInClass sono presenti quei codici epifania che sono presenti nella dataSet completto ma per cui non vi è una classificazione in db_class. Questo spiega perchè la lunghezza finale del database contentente i pazienti sia con una classificazione in db_class che i dati in dataSet è di 823."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rinomino la colonna CAD poichè è quella contenente la classificazione e in questo modo sono \n",
    "#anche sicuro che non perderò l'allineamento riga del dataset con riga del vettore con la classificazione\n",
    "dataSet = db\n",
    "dataSet.rename(columns = {'CAD (0: stenosi 0%; 1: stenosi 1-69%; 2: stenosi 70-100% o significativa)':'Class1'}, inplace = True)\n",
    "y_class = dataSet[\"Class1\"].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "y_class2 = dataSet[\"Class1\"]\n",
    "\n",
    "find_file(\"../File Memory/datasetFinale_NonImputato.csv\",dataSet,0)\n",
    "\n",
    "dataSet = read_csv(\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/datasetFinale_NonImputato.csv\")\n",
    "del dataSet[\"Class1\"]\n",
    "del dataSet[\"TC clinica\"]\n",
    "del dataSet[\"Equi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec7afb",
   "metadata": {},
   "source": [
    "Ottengo i due dataset \"db\" e \"dataset\", in particolare nel primo sono anche contenute tutte le colonne di entrambi i dataset mentre il secondo contiene solo gli items (quindi i pazienti) il cui codice epifinia è contenuto nel database della classe. A questo punto faccio l'imputing del database sfruttando il Miss Forest, stando però attento a:\n",
    "1) Normalizzare il dataset;\n",
    "2) Distringuere la variabili continue da quelle categoriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f11949",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_paz = dataSet[\"ID PAZIENTE\"]\n",
    "del dataSet[\"ID PAZIENTE\"]\n",
    "del dataSet[\"Sample_ID\"]\n",
    "imputer = IterativeImputer(missing_values = np.nan, estimator = ExtraTreesRegressor(n_estimators=10, random_state=0))\n",
    "Ximp = imputer.fit_transform(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f703c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_file(\"../File Memory/id_paz.csv\", id_paz, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per ora lo separo come cella in modo da non dover runnare ogni volta l'Isoforest\n",
    "datasetImputed = pd.DataFrame(Ximp, columns = dataSet.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b57145",
   "metadata": {},
   "source": [
    "Dato che dopo l'imputing perdo l'informazione del tipo delle singolo colonne, posso ricavare tale informazione dal database non imputanto visto che i due dataset (imputato e non) sono indentici dal punto di vista della struttura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a780fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = dataSet.columns[dataSet.dtypes == 'category']\n",
    "datasetImputed[cat_features] = datasetImputed[cat_features].astype(\"int\") #Converto prima le colonne che saranno categoriche in intero\n",
    "datasetImputed[cat_features] = datasetImputed[cat_features].astype(dtype = \"category\") #Dopodiche associo il tipo\n",
    "\n",
    "#Estraggo dal dataset le variabili continue\n",
    "cont_features = dataSet.columns[dataSet.dtypes != 'category']\n",
    "\n",
    "#Estraggo dal dataset le variabili categoriche\n",
    "find_file(\"../File Memory/datasetFinale_Imputato.csv\",datasetImputed.copy(),0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387019e",
   "metadata": {},
   "source": [
    "Dopo aver ottenuto il dataset imputato lo salvo in modo da poterlo conservare per utilizzi successivi senza dover ogni volta rieseguire l'intero algoritmo. Successivamente valuto la bontà dell'imputing tramite un confronto tra db imputato e db non imputato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac926df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mi salvo una copia del dataset senza dummy\n",
    "dataNDumm = datasetImputed\n",
    "#Trasformo le variabili categoriche in dummy, in modo tale che dopo posso effettuare lo scaling\n",
    "XcatFeatures =  datasetImputed[cat_features]\n",
    "XcatND = XcatFeatures #servirà una copia senza dummy variable\n",
    "\n",
    "XcatFeatures = pd.get_dummies(XcatFeatures, columns=cat_features.delete(0), drop_first=True)\n",
    "XcatFeatures[\"ID PAZIENTE\"] = id_paz #aggiungo questa colonna in modo poi da poter rifare il merge con il dataframe continuo\n",
    "\n",
    "#Isolo il dataframe continuo\n",
    "XcontFeatures = datasetImputed[cont_features]\n",
    "XcontFeatures[\"ID PAZIENTE\"] = id_paz\n",
    "\n",
    "#Faccio il merge tra i due dataset scomposti\n",
    "datasetImputed = pd.merge(left=XcontFeatures, right=XcatFeatures, how='outer', on='ID PAZIENTE')\n",
    "del datasetImputed[\"ID PAZIENTE\"]\n",
    "\n",
    "#Recupero il nome delle categoriche e continue\n",
    "cat_features = datasetImputed.columns[datasetImputed.dtypes == 'uint8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per prima cosa vado ad eseguire lo scaling del mio dataset\n",
    "dataImputed = datasetImputed\n",
    "X = datasetImputed.values #trasformo il dataframe in una matrix\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "datasetImputed = pd.DataFrame(X_scaled, columns = datasetImputed.columns)\n",
    "#Ristabilisco il tipo delle colonne\n",
    "datasetImputed[cat_features] = datasetImputed[cat_features].astype(\"int\") #Converto prima le colonne che saranno categoriche in intero\n",
    "datasetImputed[cat_features] = datasetImputed[cat_features].astype(dtype = \"category\") #Dopodiche associo il tipo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1ceac",
   "metadata": {},
   "source": [
    "Dopo aver ottenuto il dataset completo e pronto per l'analisi, eseguo la parte di algoritmo che mi permette di poter valutare le lenti che sono stasticamente significative nel fare il washout della classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poichè il dataSet X è eterogeneo affinchè il TDA funzioni in modo adeguato è necessario che X venga reso omogeneo \n",
    "# attraverso una misura di similarità, in questo caso similarità coseno e gower\n",
    "X1 = cosine_similarity(X_scaled,X_scaled)\n",
    "X2 = gw.gower_matrix(X_scaled)\n",
    "\n",
    "#Salvo le matrici delle distanze coseno e gower in formato csv \n",
    "np.savetxt(\"GowerMatrix.csv\", X2, delimiter=\",\")\n",
    "np.savetxt(\"CosineMatrix.csv\", X1, delimiter=\",\")\n",
    "np.savetxt(\"Class.csv\", y_class, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dataSet.columns\n",
    "feature_names = feature_names.delete(len(feature_names)-1)\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "# We create a custom 1-D lens with Isolation Forest\n",
    "model = ensemble.IsolationForest(random_state=rnd)\n",
    "lens1 = model.fit(X2)\n",
    "lens1 = model.decision_function(X2).reshape((X2.shape[0], 1))\n",
    "IsoForest_lens = pd.Series(np.concatenate(lens1))  #Cast into Pandas Series\n",
    "\n",
    "# We create another 1-D lens with L2-norm\n",
    "lens2 = mapper.fit_transform(X2, projection=\"l2norm\", distance_matrix = None, scaler = None)\n",
    "L2Norm_lens = pd.Series(np.concatenate(lens2))  #Cast into Pandas Series\n",
    "\n",
    "# We create another 1-D lens with one component of PCA\n",
    "lens3 = mapper.fit_transform(X2, projection=sklearn.decomposition.PCA(n_components=1, random_state=rnd), distance_matrix = None, scaler = None)\n",
    "PCA_lens = pd.Series(np.concatenate(lens3))  #Cast into Pandas Series\n",
    "\n",
    "\n",
    "# We create another 1-D lens with with one component of UMAP\n",
    "lens4 = mapper.project(X2, projection=umap.UMAP(n_components=1, random_state=rnd), distance_matrix = None, scaler = None)\n",
    "Umap1_lens = pd.Series(np.concatenate(lens4))  #Cast into Pandas Series\n",
    "\n",
    "# We create another 1-D lens with with two component of UMAP\n",
    "lens5 = mapper.project(X2, projection=umap.UMAP(n_components=2, random_state=rnd), distance_matrix = None, scaler = None)\n",
    "Umap2_lens = pd.Series(np.concatenate(lens5))  #Cast into Pandas Series\n",
    "\n",
    "\n",
    "# We create another 1-D lens with with one component of TSNE\n",
    "lens6 = mapper.project(X2, projection= TSNE(n_components=1, random_state=rnd,perplexity = 1),distance_matrix = None, scaler = None)\n",
    "Tsne1_lens = pd.Series(np.concatenate(lens6))\n",
    "\n",
    "# We create another 2-D lens with with two component of TSNE\n",
    "lens7 = mapper.project(X2, projection= TSNE(n_components=2, random_state=rnd, perplexity = 1,), distance_matrix = None, scaler = None)\n",
    "Tsne2_lens = pd.Series(np.concatenate(lens7))\n",
    "\n",
    "\n",
    "\n",
    "#Creo un dizionario con le lenti create\n",
    "lens = {'IsoForest_lens': [IsoForest_lens],\n",
    "        'L2Norm_lens':[L2Norm_lens],\n",
    "        'PCA_lens':[PCA_lens],\n",
    "        'Umap1_lens': [Umap1_lens],\n",
    "        'Umap2_lens':[Umap2_lens],\n",
    "        'Tsne1_lens':[Tsne1_lens],  \n",
    "        'Tsne2_lens':[Tsne2_lens]}\n",
    "\n",
    "    \n",
    "#L'idea è quella di andare a generare le combinazioni in maniera automatica a partire dal dizionario\n",
    "#Salvo il nome delle lenti da combinare in una variabile\n",
    "lens_name = list(lens.keys()) \n",
    "n_lens = len(lens_name)\n",
    "\n",
    "#Creo due cicli for in modo da combinare le varie lenti (potrò combinare solo lenti 1D con altre lenti 1D)\n",
    "for a in lens_name:\n",
    "    for b in lens_name[lens_name.index(a)+1:]:\n",
    "        #Crea una chiave con solo a\n",
    "        dim1 = int(np.array(lens[a]).size)\n",
    "        dim2 = int(np.array(lens[b]).size)\n",
    "        #La combinazione tra a e b viene fatta soltanto se entrambi sono di dimensione 1\n",
    "        if dim1 <= y_class.size & dim2 <= y_class.size:\n",
    "            key = a[0:a.find('_')] + '_' + b[0:b.find('_')] + '_lens'\n",
    "            lens[key] = np.c_[lens[a], lens[b]]\n",
    "            \n",
    "lens_dict = lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ea514",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_accept = [];\n",
    "print(\"Eseguo il test di Shapiro per verificare la normalità della lente splittanta rispetto alla classe e successivamente la\\\n",
    "      valuto attravero il test di Mann Whitney la capacità della lente di distinguere e descrivere le due classi.\")\n",
    "for i in lens_name:\n",
    "    dim = int(np.array(lens[i]).size)\n",
    "    if dim == y_class.size:\n",
    "        df2 = pd.DataFrame(np.array(lens[i]).reshape(-1,1), columns = [i])\n",
    "        df2['class'] = y_class\n",
    "        lens_class0 = (df2[i].loc[df2[\"class\"] == 0])\n",
    "        lens_class1 = (df2[i].loc[df2[\"class\"] == 1])\n",
    "        #Grafico la distribuzione campionaria rispetto a quella Gaussiana rispetto alla classe\n",
    "        plt.figure(figsize = (6,6))\n",
    "        sns.distplot(lens_class0, kde = True, color = \"c\")\n",
    "        sns.distplot(lens_class1, kde = True, color = \"y\")\n",
    "        #plt.plot(x1, scy.norm.pdf(x1, mu1, sigma1))\n",
    "        plt.legend(['Hist Class_0','Hist Class_1','Dist Campionaria Class_0', 'Dist Campionaria Class_1'])\n",
    "        plt.title(i + ' vs Class 0 and Class 1')\n",
    "        plt.savefig('C:/Users/Antonio Montanaro/Desktop/' + i + 'vsClass0_1.png', format = \"png\", dpi = 1000)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        #Eseguo due test statistici (poi  sceglierò soltanto uno)\n",
    "        #Test di Shapiro molto buono anche per campioni piccoli\n",
    "        print('1) Eseguo test statistico di Shapiro per verificare la normalità')\n",
    "        print('-- Il test rifiuta ipotesi H0 che il campione provenga da una distribuzione normale --')\n",
    "        tShap0, pShap0 = scy.shapiro(lens_class0)\n",
    "        tShap1, pShap1 = scy.shapiro(lens_class1)\n",
    "        if pShap0<=0.05 and pShap1<=0.05:\n",
    "            print('Rifiuto H0. I campioni non provengono da una distribuzione normale')\n",
    "            print(str(pShap0))\n",
    "            print(str(pShap1))\n",
    "        else:\n",
    "            print('I campioni provengono da una distribuzione normale')    \n",
    "        \n",
    "        \n",
    "            \n",
    "        print(\"2) Eseguo il test statistico di MannWhitney per confrontare i due campioni.\")\n",
    "        print('-- Il test rifiuta ipotesi H0 che i due campioni provengano dalla stessa distribuzione --')\n",
    "        \n",
    "        t, p = scy.mannwhitneyu(lens_class0,lens_class1)\n",
    "        print(t)\n",
    "        if p <= 0.05:\n",
    "            lens_accept.append(i)\n",
    "           \n",
    "            print(i + ' è stata accetta')\n",
    "            print(str(p*100) + \"%\")\n",
    "        else:\n",
    "            print(i + ' non è stata accetta')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac23636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le lenti accettate sono:\")\n",
    "for i in lens_accept:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9353ce",
   "metadata": {},
   "source": [
    "Dopo aver eseguito una prima classificazione delle lenti tramite test statistico decido di andare ad eseguire una fase più accurata della lente migliore tramite Greedy-Search tra le lenti:\n",
    "- Tsne2D\n",
    "- Umap2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a8f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Valutazione più accurata della lente Tsne\n",
    "perp = np.arange(15,55,10) \n",
    "learning_rate = np.arange(300,1200,400)\n",
    "n_iter= np.arange(500,3000, 1000)\n",
    "entropies_dict_Tsne = {}\n",
    "for i in perp:\n",
    "    for j in learning_rate:\n",
    "        for k in n_iter:\n",
    "            lensX = mapper.project(X2, projection= TSNE(n_components=2, random_state=17489, perplexity = i, learning_rate=j, n_iter=k),distance_matrix = None, scaler = None)\n",
    "            Tsne2_lens = pd.DataFrame(lensX)\n",
    "            scomplex =  mapper.map(Tsne2_lens,\n",
    "                                    X2,\n",
    "                                    cover=km.Cover(n_cubes=16, perc_overlap=0.65),  #Modifica rispetto al github causa versione del mapper\n",
    "                                    clusterer=sklearn.cluster.KMeans(n_clusters=2,random_state=1618033))\n",
    "            #Funzione che dato il grafo calcola l'entropia\n",
    "            entropies_dict_Tsne[\"Tsne2D_perp=\",i,\"_learnRate=\",j,\"_nIter=\",k] = entropy_count(scomplex) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c36525",
   "metadata": {},
   "source": [
    "Dalla greedy search eseguita nella cella sopra si evince come i parametri che vanno a minimizzare l'entropia sono:\n",
    "- Perplexity = 45\n",
    "- Learing Rate = 300\n",
    "- Number Iteration = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e293a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_key = min(entropies_dict_Tsne, key=lambda key: entropies_dict_Tsne[key])\n",
    "print(min_key)\n",
    "\n",
    "lensX = mapper.project(X2, projection= TSNE(n_components=2, random_state=17489, perplexity = 45, learning_rate=300, n_iter=1500),distance_matrix = None, scaler = None)\n",
    "Tsne2_lens_final = pd.DataFrame(lensX)\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "plt.title(\"Scatter Plot of Tsne Lens\")\n",
    "plt.suptitle(\"Tuned Parameters: Perpl = 45 - NIter = 1500 - LearnRate = 300\")\n",
    "plt.scatter(Tsne2_lens_final[0], Tsne2_lens_final[1], c = y_class)\n",
    "plt.legend(['Class 0','Class 1'])\n",
    "plt.savefig('Prova11232.png', format='png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add0960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lente più significativa che analizzo con grid search è: Umap\n",
    "min_dist = [0.2,0.3,0.5,0.7,0.8]\n",
    "n_neighbors=[5,10,25,50,120,150,200]\n",
    "entropies_dict_Umap = {}\n",
    "for i in n_neighbors:\n",
    "    for j in min_dist:\n",
    "        lensX = mapper.project(X2, projection=umap.UMAP(n_components=2, random_state=17489, n_neighbors= i, min_dist=j),distance_matrix = None, scaler = None)\n",
    "        Umap2_lens = pd.DataFrame(lensX)\n",
    "        scomplex =  mapper.map(Umap2_lens,\n",
    "                                    X2,\n",
    "                                    cover=km.Cover(n_cubes=16, perc_overlap=0.65),  \n",
    "                                    clusterer=sklearn.cluster.KMeans(n_clusters=2,random_state=1618033))\n",
    "        #Funzione che dato il grafo calcola l'entropia\n",
    "        entropies_dict_Umap[\"Umap2D_MinDist=\",j,\"_Nneigh=\",i] = entropy_count(scomplex) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a373e0",
   "metadata": {},
   "source": [
    "Dalla greedy search eseguita nella cella sopra si evince come i parametri che vanno a minimizzare l'entropia sono:\n",
    "- Min Distance = 0.3\n",
    "- Number Neighbors = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_key = min(entropies_dict_Umap, key=lambda key: entropies_dict_Umap[key])\n",
    "print(min_key)\n",
    "\n",
    "lensX = mapper.project(X2, projection=umap.UMAP(n_components=2, random_state=rnd, n_neighbors= 25, min_dist=0.3),distance_matrix = None, scaler = None)\n",
    "Umap2_lens_final = pd.DataFrame(lensX)\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "plt.title(\"Scatter Plot of Umap Lens\")\n",
    "plt.suptitle(\"Tuned Parameters: Min Distance = 0.3 - N Neighbors = 25\")\n",
    "plt.scatter(Umap2_lens_final[0], Umap2_lens_final[1], c = y_class)\n",
    "plt.legend(['Class 0','Class 1'])\n",
    "plt.savefig('Prova11232.png', format='png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea407a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_key = min(entropies_dict_Tsne, key=lambda key: entropies_dict_Tsne[key])\n",
    "print(\"Valore di minimo di Entropia per Tsne2D: \",entropies_dict_Tsne[min_key])\n",
    "\n",
    "min_key = min(entropies_dict_Umap, key=lambda key: entropies_dict_Umap[key])\n",
    "print(\"Valore di minimo di Entropia per Umap2D: \",entropies_dict_Umap[min_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a97f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "\n",
    "pathAbs = str(pathlib.Path().absolute()) \n",
    "document = Document()\n",
    "\n",
    "document.add_heading('Lens Evaluation', 0)\n",
    "\n",
    "p = document.add_paragraph('Dopo aver generato le diverse lenti, si è passati alla valutazione tramire appositi test-statistici. \\n')\n",
    "p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "p.add_run(\" L'obiettivo è quello di utilizzare dei test statistici per poter determinare se una lente \\\n",
    "abbia o meno capacità discriminatorie delle classi. I metodi statistici parametrici presumono che \\\n",
    "i dati abbiano una distribuzione nota e specifica, distribuzione gaussiana nel caso del \\\n",
    "t-test ad esempio. Perciò, prima di proseguire nell’esecuzione del t-test sarà necessario \\\n",
    "verificare l’ipotesi di normalità. Ci sono fondamentalmente due strategie per poter valutare \\\n",
    "tale ipotesi, la prima strategia è quella grafica mentre la seconda è quella di applicare un \\\n",
    "test statistico.\")\n",
    "\n",
    "p.add_run(\"Per quanto riguarda l’approccio grafico, tra i vari strumenti, possiamo andare ad utilizzare \\\n",
    "l’istogramma che ci permette di visualizzare seppur in maniera grossolana, se il campione di \\\n",
    "nostro interesse è aderente o meno alla distribuzione normale.\")\n",
    "\n",
    "p.add_run(\"Sebbene tramite quest'analisi sia possibile concludere se le distribuzioni siano o meno gaussiane \\\n",
    "per dimostrare in maniera analitica quanto si osserva, è stato eseguito un test statistico di normalità ovvero \\\n",
    "il test di Shapiro-Wilk. Quest'ultimo, è uno dei test più potenti per la verifica della normalità, soprattutto \\\n",
    "per piccoli campioni. La verifica della normalità avviene confrontando due stimatori alternativi della varianza: \\\n",
    "uno stimatore non parametrico basato sulla combinazione lineare ottimale della statistica d'ordine di una variabile \\\n",
    "aleatoria normale al numeratore, e il consueto stimatore parametrico, ossia la varianza campionaria, al denominatore.\")\n",
    "\n",
    "p.add_run(\"\")\n",
    " \n",
    "\n",
    "document.add_heading('Valutazione ipotesi normalità', level=1)\n",
    "#Creo questo sezione di documento in modo automatico \n",
    "for i in lens_name:\n",
    "    dim = int(np.array(lens[i]).size)\n",
    "    if dim <= y_class.size:\n",
    "        d1 = pd.Series(lens[i])\n",
    "        df2 = pd.DataFrame(np.array(lens[i]).reshape(-1,1), columns = [i])\n",
    "        df2['class'] = y_class\n",
    "        lens_class0 = (df2[i].loc[ df2['class'] == 0])\n",
    "        lens_class1 = (df2[i].loc[ df2['class'] == 1])\n",
    "        \n",
    "        document.add_heading(i, level=2)\n",
    "\n",
    "        document.add_picture(pathAbs + '/LensDistPlot/OriginalData' + i + 'vsClass0_1.png', width=Inches(5))\n",
    "        \n",
    "        document.add_paragraph(\"Test di normalità Shapiro-Wilk\")\n",
    "        #Test di Shapiro-Wilk molto buono anche per campioni piccoli\n",
    "        tShap0, pShap0 = scy.shapiro(lens_class0)\n",
    "        tShap1, pShap1 = scy.shapiro(lens_class1)\n",
    "        if pShap0<=0.05 and pShap1<=0.05:\n",
    "            document.add_paragraph(\"Il test rifiuta l'ipotesi nulla H0 ovvero che i due campioni provengano \\\n",
    "da una distribuzione normale con un alpha dello 0.05. \\n P-value del \" + \"{:.18f}\".format((pShap0*100)) + \" % \\\n",
    "per il campione di classe 0. \\n P-value del \" + \"{:.18f}\".format((pShap1*100)) + \" % per il campione di classe 1.\")\n",
    "        else:\n",
    "            document.add_paragraph(\"Il test non rifiuta l'ipotesi nulla H0 ovvero che i due campioni provengano \\\n",
    "da una distribuzione normale con un alpha dello 0.05. I campioni, dunque, provengono da una distribuzione \\\n",
    "normale.\")\n",
    "\n",
    "\n",
    "document.add_heading('Valutazione Lenti', level=1)\n",
    "document.add_paragraph(\"Poichè come visto prima nessun campione è distribuito normalmente, la scelta di applicare \\\n",
    "test parametri non è percorribile. Di conseguenza per poter valutare le capacità della lente di discriminare le \\\n",
    "classi possiamo applicare il test MannWhitney ovvero un test non parametrico per dati non appaiti. \\\n",
    "In particolare, l'ipotesi nulla nel test di Mann-Whitney è quella che i due campioni siano tratti da una popolazione \\\n",
    "singola, e che dunque per questa ragione le loro distribuzioni di probabilità siano uguali. L'ipotesi alternativa \\\n",
    "è che uno dei campioni sia più grande in maniera stocastica. Questo richiede che i due campioni siano statisticamente \\\n",
    "indipendenti e che le osservazioni siano almeno ordinali, o quantitative continue o discrete, perciò questo test si presta \\\n",
    "molto per il nostro scopo, ovvero per il confronto dei due campioni campioni ottenuti dalle diverse lente e \\\n",
    "stratificati rispetto alla classe. Il test lavora rifuitando l'ipotesi nulla (H0: i due campioni provengono dalla \\\n",
    "stessa distribuzione). L'ipotesi alternativa è che uno dei campioni sia più grande in maniera stocastica.\")\n",
    "                       \n",
    "document.add_paragraph(\"Seguono i risultati del test per ogni lente: \\n\")\n",
    "table = document.add_table(rows=1, cols=3)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Lent'\n",
    "hdr_cells[1].text = 'P-value'\n",
    "hdr_cells[2].text = 'Rifiuto H0 [S/N]'\n",
    "for i in lens_name:\n",
    "    dim = int(np.array(lens[i]).size)\n",
    "    if dim <= y_class.size:\n",
    "        d1 = pd.Series(lens[i])\n",
    "        df2 = pd.DataFrame(np.array(lens[i]).reshape(-1,1), columns = [i])\n",
    "        df2['class'] = y_class\n",
    "        lens_class0 = (df2[i].loc[ df2['class'] == 0])\n",
    "        lens_class1 = (df2[i].loc[ df2['class'] == 1])\n",
    "        t, p = scy.mannwhitneyu(lens_class0,lens_class1)\n",
    "        row_cells = table.add_row().cells\n",
    "        row_cells[0].text = i\n",
    "        row_cells[1].text = str(round(p*100,6)) + \" %\" \n",
    "        if p <= 0.05:\n",
    "            row_cells[2].text = 'S'\n",
    "        else:\n",
    "            row_cells[2].text = 'N'\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('LensEvaluation1.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20122a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A questo punto vado ad attuare una greedy search per trovare i parametri di risoluzione che mi minimizzano la lente.\n",
    "#Questo lo andrò a fare sia per le lente Umap che per la  lente Tsne.\n",
    "#Lente Tsne - Algoritmo di Cluster Agglomerativo Singol \n",
    "#Creo delle mappe che tengono in considerazione soltanto la lente Tsne che ha come parametri min_dist=0.8, n_neighbors=10\n",
    "#Create a vector of different perc_overlap and n_cubes\n",
    "perc_over = np.arange(0.3,0.8,0.05)\n",
    "n_cubes = np.arange(12,22,2)\n",
    "heartFail_dict = {0: 'Class_0', 1: 'Class_1'}\n",
    "\n",
    "Tsne2D_Final_Agglomerative = {}\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Tsne2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.AgglomerativeClustering(linkage=\"single\", distance_threshold=None),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j) #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "            )\n",
    "            \n",
    "            title =  \"Lens = Tsne2D - Clustering = Agglomerative - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Tsne2D_Final_Agglomerative[\"Lens = Tsne2D - Clustering = Agglomerative - N_cubes =\" + str(i) +\"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiTsne\\ClusterAgglomerative\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensTsne2D - Agglormerative - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiTsne\\ClusterAgglomerative\\HistImage\"+ str(count)+\".jpeg\")\n",
    "    \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color, external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiTsne\\ClusterAgglomerative\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path,node_color, internal, internal_color)\n",
    "\n",
    "\n",
    "Tsne2D_Final_DBSCAN = {}\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Tsne2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.DBSCAN(),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j)  #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "            )\n",
    "            \n",
    "            title =  \"Lens = Tsne2D - Clustering = DBSCAN - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Tsne2D_Final_DBSCAN[\"Lens = Tsne2D - Clustering = DBSCAN - N_cubes =\" + str(i)+ \"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiTsne\\DBSCAN\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensTsne2D - DBSCAN - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiTsne\\DBSCAN\\HistImage\"+ str(count)+\".jpeg\")\n",
    "            \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color, external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiTsne\\DBSCAN\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path,node_color, internal, internal_color)\n",
    "\n",
    "Tsne2D_Final_KMeans = {}\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Tsne2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.KMeans(n_clusters=2, random_state=1618033),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j)  #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "            )\n",
    "\n",
    "            title =  \"Lens = Tsne2D - Clustering = KMeans - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Tsne2D_Final_KMeans[\"Lens = Tsne2D - Clustering = KMeans - N_cubes =\" + str(i) +\"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiTsne\\ClusterKMeans\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensTsne2D - KMeans - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiTsne\\ClusterKMeans\\HistImage\"+ str(count)+\".jpeg\")\n",
    "            \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color, external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiTsne\\ClusterKMeans\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path, node_color, internal, internal_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lente Umap - Algoritmo di Cluster Agglomerativo Singol \n",
    "#Creo delle mappe che tengono in considerazione soltanto la lente Tsne che ha come parametri min_dist=0.8, n_neighbors=10\n",
    "#Create a vector of different perc_overlap and n_cubes\n",
    "perc_over = np.arange(0.3,0.8,0.05)\n",
    "n_cubes = np.arange(12,22,2)\n",
    "\n",
    "Umap2D_Final_Agglomerative = {}\n",
    "\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Umap2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.AgglomerativeClustering(linkage=\"single\", distance_threshold=None),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j) #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "                \n",
    "            )\n",
    "            \n",
    "            title =  \"Lens = Umap2D - Clustering = Agglomerative - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Umap2D_Final_Agglomerative[\"Lens = Umap2D - Clustering = Agglomerative - N_cubes =\" + str(i)+ \"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiUmap\\ClusterAgglomerative\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensUmap2D - Agglormerative - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiUmap\\ClusterAgglomerative\\HistImage\"+ str(count)+\".jpeg\")\n",
    "            \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color, external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiUmap\\ClusterAgglomerative\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path, node_color, internal, internal_color)\n",
    "            \n",
    "\n",
    "Umap2D_Final_DBSCAN = {}\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Umap2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.DBSCAN(),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j)  #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "            )\n",
    "            \n",
    "            title =  \"Lens = Umap2D - Clustering = DBSCAN - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Umap2D_Final_DBSCAN[\"Lens = Umap2D - Clustering = DBSCAN - N_cubes =\" + str(i) +\"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiUmap\\DBSCAN\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensUmap2D - DBSCAN - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiUmap\\DBSCAN\\HistImage\"+ str(count)+\".jpeg\")\n",
    "            \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color, external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiUmap\\DBSCAN\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path, node_color, internal, internal_color)\n",
    "            \n",
    "Umap2D_Final_KMeans = {}\n",
    "count = 0\n",
    "for i in n_cubes:\n",
    "    for j in perc_over:\n",
    "            scomplex = mapper.map(\n",
    "                Umap2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.KMeans(n_clusters=2, random_state=1618033),\n",
    "                cover=km.Cover(n_cubes=i, perc_overlap=j)  #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    "            )\n",
    "\n",
    "            title =  \"Lens = Umap2D - Clustering = KMeans - Entropia = \" + str(entropy_count(scomplex))\n",
    "            Umap2D_Final_KMeans[\"Lens = Umap2D - Clustering = KMeans - N_cubes =\" + str(i)+ \"- %_overlap =\" + str(j)] = entropy_count(scomplex)\n",
    "            count = count + 1\n",
    "            fw_graph = do_graph(scomplex,title,y_class, pl_brewer)\n",
    "            fw_graph.write_image(\"ImageFinals\\ImmaginiUmap\\ClusterKMeans\\Image\"+ str(count)+\".jpeg\" ,width=800, height=800)\n",
    "            do_hist_counter(scomplex,\"LensUmap2D - KMeans - N_cub=\" + str(i) +\"- %_overlap =\" + str(j) + \"\\n\" + \"Distribuzione della dimensione dei nodi\", \"ImageFinals\\ImmaginiUmap\\ClusterKMeans\\HistImage\"+ str(count)+\".jpeg\")\n",
    "            \n",
    "            color_class = do_color_class(scomplex,y_class,heartFail_dict,pl_brewer)\n",
    "            #Utilizzo una funzione del kmapper per trasformarel'output del mapper in una rete NetworkX\n",
    "            G = km.adapter.to_nx(scomplex)\n",
    "            node_color, internal, internal_color,external = community_searching(G)\n",
    "            path = \"ImageFinalsNetworkX\\ImmaginiUmap\\ClusterKMeans\\Image\"+ str(count)+\".jpeg\"\n",
    "            do_graph_NetworkX(G,color_class, path, node_color, internal, internal_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_dict(dictionary, path):\n",
    "    a_file = open(path, \"wb\")\n",
    "    pickle.dump(dictionary, a_file)\n",
    "    a_file.close()\n",
    "    \n",
    "print(\"Tsne Lens\")\n",
    "min_key = min(Tsne2D_Final_Agglomerative, key=lambda key: Tsne2D_Final_Agglomerative[key])\n",
    "print(\"Valore di minimo di Entropia per Tsne2D: \",Tsne2D_Final_Agglomerative[min_key],\"\\n\", min_key)\n",
    "save_dict(Tsne2D_Final_Agglomerative,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Tsne2D_Final_Agglomerative.pkl\")\n",
    "\n",
    "min_key = min(Tsne2D_Final_KMeans, key=lambda key: Tsne2D_Final_KMeans[key])\n",
    "print(\"Valore di minimo di Entropia per Tsne2D: \",Tsne2D_Final_KMeans[min_key],\"\\n\", min_key)\n",
    "save_dict(Tsne2D_Final_KMeans,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Tsne2D_Final_KMeans.pkl\")\n",
    "\n",
    "min_key = min(Tsne2D_Final_DBSCAN, key=lambda key: Tsne2D_Final_DBSCAN[key])\n",
    "print(\"Valore di minimo di Entropia per Tsne2D: \",Tsne2D_Final_DBSCAN[min_key],\"\\n\", min_key)\n",
    "save_dict(Tsne2D_Final_DBSCAN,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Tsne2D_Final_DBSCAN.pkl\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Umap Lens\")\n",
    "min_key = min(Umap2D_Final_Agglomerative, key=lambda key: Umap2D_Final_Agglomerative[key])\n",
    "print(\"Valore di minimo di Entropia per Umap2D: \",Umap2D_Final_Agglomerative[min_key],\"\\n\", min_key)\n",
    "save_dict(Umap2D_Final_Agglomerative,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Umap2D_Final_Agglomerative.pkl\")\n",
    "\n",
    "\n",
    "min_key = min(Umap2D_Final_KMeans, key=lambda key: Umap2D_Final_KMeans[key])\n",
    "print(\"Valore di minimo di Entropia per Umap2D: \",Umap2D_Final_KMeans[min_key],\"\\n\", min_key)\n",
    "save_dict(Umap2D_Final_KMeans,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Umap2D_Final_KMeans.pkl\")\n",
    "\n",
    "\n",
    "min_key = min(Umap2D_Final_DBSCAN, key=lambda key: Umap2D_Final_DBSCAN[key])\n",
    "print(\"Valore di minimo di Entropia per Umap2D: \",Umap2D_Final_DBSCAN[min_key],\"\\n\", min_key)\n",
    "save_dict(Umap2D_Final_DBSCAN,\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/File Memory/Dictionary/Umap2D_Final_DBSCAN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691bb31",
   "metadata": {},
   "source": [
    "Prendiamo in analisi Tsne2D con:\n",
    "- % Overlap = 0.55\n",
    "- N cubes = 18\n",
    "- Algoritmo CLustering = KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac63943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eseguo i BoxPlot per poter confrontare le entropie dei diversi metodi di clustering con successivo test statistico\n",
    "dataDictionaryLens = pd.DataFrame()\n",
    "dataDictionaryLens[\"Tsne2D_Agglomerative\"] = Tsne2D_Final_Agglomerative.values()\n",
    "dataDictionaryLens[\"Tsne2D_KMeans\"] = Tsne2D_Final_KMeans.values()\n",
    "dataDictionaryLens[\"Tsne2D_DBSCAN\"] = Tsne2D_Final_DBSCAN.values()\n",
    "dataDictionaryLens[\"Umap2D_Agglomerative\"] = Umap2D_Final_Agglomerative.values()\n",
    "dataDictionaryLens[\"Umap2D_KMeans\"] = Umap2D_Final_KMeans.values()\n",
    "dataDictionaryLens[\"Umap2D_DBSCAN\"] =  prova\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3109b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=dataDictionaryLens)\n",
    "plt.title(\"Boxplot Entropie diversi algoritmi di clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verico la normalità dei vari vettori di entropia\n",
    "for i in dataDictionaryLens.columns:\n",
    "    fstat, pval =scy.shapiro(dataDictionaryLens[i])\n",
    "    if pval <= 0.05:\n",
    "        print(i + \" è distribuito normalemente\")\n",
    "    else:\n",
    "        print(i + \"non è distribuito normalemente\")\n",
    "#Poichè tutti i vettori sono distribuiti normalmente posso applicare il test anova che va a verificare se le medie dei\n",
    "#vettori sono uguali, se dovessi rifiutare l'ipotesi nulla potrei dire che le medie sono diverse ma non saprei dire quale\n",
    "#è significativamente diversa, occorrera quindi eseguire un nuovo test.\n",
    "print(\"\")\n",
    "fvalue, pvalue =scy.f_oneway(dataDictionaryLens[\"Tsne2D_Agglomerative\"], dataDictionaryLens[\"Tsne2D_KMeans\"], dataDictionaryLens[\"Tsne2D_DBSCAN\"], dataDictionaryLens[\"Umap2D_Agglomerative\"], dataDictionaryLens[\"Umap2D_KMeans\"], dataDictionaryLens[\"Umap2D_DBSCAN\"])\n",
    "if pvalue<0.05:\n",
    "    print(\"Le medie dei vettori sono significativamente diverse! Occorre eseguire un test di Tukey!\")\n",
    "else:\n",
    "    print(\"Le medie non sono significativemente diverse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ae917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparo il dataframe per il test statistico\n",
    "lista = dataDictionaryLens[\"Tsne2D_Agglomerative\"].tolist() + dataDictionaryLens[\"Tsne2D_KMeans\"].tolist()+ dataDictionaryLens[\"Tsne2D_DBSCAN\"].tolist() + dataDictionaryLens[\"Umap2D_Agglomerative\"].tolist() + dataDictionaryLens[\"Umap2D_KMeans\"].tolist()+ dataDictionaryLens[\"Umap2D_DBSCAN\"].tolist() \n",
    "df = pd.DataFrame({'score': lista,\n",
    "                   'group': np.repeat(['Tsne_Aggl', 'Tsne_KMeans','Tsne_DBSCAN','Umap_Aggl','Umap_KMeans','Umap_DBSCAN'], repeats=len(dataDictionaryLens))}) \n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=df['score'],groups=df['group'],alpha=0.05)\n",
    "print(tukey)\n",
    "print(\"I risultati devono essere interpretati facendo riferimento al meandiff, dove questo è piccolo indica una bassa\\\n",
    " differenza\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b243b0c",
   "metadata": {},
   "source": [
    "Dai risultati sopra risulta evidente che il clustering con DBSCAN sia quello più scadente. Quindi si prenderà in considerazione il KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7efc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scomplex\n",
    "lensX = mapper.project(X2, projection= TSNE(n_components=2, random_state=17489, perplexity = 45, learning_rate=300, n_iter=1500),distance_matrix = None, scaler = None)\n",
    "Tsne2_lens_final = pd.DataFrame(lensX)\n",
    "\n",
    "scomplex = mapper.map(\n",
    "                Tsne2_lens_final, \n",
    "                X2, \n",
    "                clusterer=sklearn.cluster.KMeans(n_clusters=2, random_state=1618033),\n",
    "                cover=km.Cover(n_cubes=18, perc_overlap=0.55)  #j e k sono rispettivamente gli elementi dei vettori 'n_cubes' e 'perc_overlap'\n",
    ")\n",
    "    \n",
    "title =  \"Lens = Tsne2D - Clustering = KMeans - Entropia = \" + str(entropy_count(scomplex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex,color_function_name='Distance to x-min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10af361",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta = dataImputed[\"Età\"]          #Prendo il vettore eta non scalato                               \n",
    "Age_scaled = datasetImputed[\"Età\"]#Prendo il vettore età scalato\n",
    "\n",
    "anniFumo = dataImputed[\"Anni da fumatore\"]           #Prendo il vettore eta non scalato                               \n",
    "anniFumo_scaled = datasetImputed[\"Anni da fumatore\"] #Prendo il vettore età scalato\n",
    "\n",
    "peso = dataImputed[\"Peso \"]           #Prendo il vettore eta non scalato                               \n",
    "peso_scaled = datasetImputed[\"Peso \"] #Prendo il vettore età scalato\n",
    "\n",
    "Gamma = dataImputed[\"Gamma-GT \"]         #Prendo il vettore eta non scalato                               \n",
    "Gamma_scaled = datasetImputed[\"Gamma-GT \"]   #Prendo il vettore età scalato\n",
    "\n",
    "HDLcolesterolo = dataImputed[\"Colesterolo HDL \"]             #Prendo il vettore eta non scalato                               \n",
    "HDLcolesterolo_scaled = datasetImputed[\"Colesterolo HDL \"]   #Prendo il vettore età scalato\n",
    "\n",
    "presSistolica = dataImputed[\"Pressione arteriosa sistolica \"]           #Prendo il vettore eta non scalato                               \n",
    "presSistolica_scaled = datasetImputed[\"Pressione arteriosa sistolica \"]   #Prendo il vettore età scalato\n",
    "\n",
    "LDLcolesterolo = dataImputed[\"Colesterolo LDL \"]           #Prendo il vettore eta non scalato                               \n",
    "LDLcolesterolo_scaled = datasetImputed[\"Colesterolo LDL \"]   #Prendo il vettore età scalato\n",
    "\n",
    "\n",
    "risk_score = pd.read_csv(\"C:/Users/Antonio Montanaro/Desktop/Tesi Magistrale/Risk_score.csv\")\n",
    "fw_rischio = do_enrinchment(scomplex,pl_brewer, risk_score[\"Risk_score\"],risk_score[\"Risk_score\"], fwn)\n",
    "VBox([HBox([fwn,fw_rischio])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87e3f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var_cat = datasetImputed[\"Il paziente soffre di diabete mellito?_1\"]\n",
    "fw_diabetic = do_enrinchment_cat(scomplex,pl_brewer, var_cat, fwn, \"of the diabetic disease\", {0:\"Cat_0\", 1:\"Cat_1\"})\n",
    "fw_malattiaCard = do_enrinchment_cat(scomplex,pl_brewer, datasetImputed['Il paziente ha mai avuto sintomi di malattia cardiaca?_1'], fwn, \"of cardiac disease\", {0:\"Cat_0\", 1:\"Cat_1\"})\n",
    "fw_sex = do_enrinchment_cat(scomplex,pl_brewer, datasetImputed[\"Sesso\"], fwn, \"of sex\", {0:\"Cat_0\", 1:\"Cat_1\"})\n",
    "\n",
    "\n",
    "VBox([HBox([fwn, fw_diabetic]), HBox([fwn, fw_malattiaCard]), HBox([fwn, fw_sex])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5f542",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a =np.random.seed(42)\n",
    "g_pos = nx.spring_layout\n",
    "G = km.adapter.to_nx(scomplex)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "\n",
    "# Grafico la stessa rete con i pacchetti NetworkX e igraph\n",
    "fig, (ax0,ax1) = plt.subplots(nrows=1, ncols=2, figsize=(14, 8))\n",
    "ax0.set_title(\"Plot with NetworkX - Class Proportion\")\n",
    "nx.colormaps = pl_brewer\n",
    "nx.draw_kamada_kawai(G, node_size=50, ax=ax0, node_color=do_color_class(scomplex,y_class,heartFail_dict,pl_brewer), cmap = 'coolwarm')\n",
    "#la color map coolwarm va dal blu (0) al rosso (1) quindi nei nodi blu vi è una prevalenza di soggetti sani\n",
    "\n",
    "G = km.adapter.to_nx(scomplex)\n",
    "node_color, internal, internal_color, external = community_searching(G)\n",
    "\n",
    "ax1.set_title(\"Plot with NetworkX - Communities\")\n",
    "nx.draw_kamada_kawai(G, node_size=50, ax=ax1, node_color= node_color, edgelist=internal, edge_color = internal_color)\n",
    "\n",
    "\n",
    "# Grafico la stessa rete con i pacchetti NetworkX e igraph\n",
    "fig, (ax0,ax1) = plt.subplots(nrows=1, ncols=2, figsize=(14, 8))\n",
    "\n",
    "ax0.set_title(\"Plot with NetworkX - Class Proportion\")\n",
    "nx.colormaps = pl_brewer\n",
    "nx.draw_kamada_kawai(G, node_size=50, ax=ax0, node_color=do_color_class(scomplex,y_class,heartFail_dict,pl_brewer), cmap = 'coolwarm')\n",
    "#la color map coolwarm va dal blu (0) al rosso (1) quindi nei nodi blu vi è una prevalenza di soggetti sani\n",
    "\n",
    "risk_score = pd.read_csv(\"C:/Users/Antonio Montanaro/Desktop/File Memory/Tesi Magistrale/Risk_score.csv\")\n",
    "color = do_color(scomplex, risk_score[\"Risk_score\"], risk_score[\"Risk_score\"], heartFail_dict,pl_brewer)\n",
    "color_rgb = []\n",
    "for i in color:\n",
    "    if i<0.03:\n",
    "        color_rgb.append(\"#1d3557\")\n",
    "    if i>0.30:\n",
    "        color_rgb.append(\"#e63946\")\n",
    "    if i >=0.03 and i<0.3:\n",
    "        color_rgb.append(\"#a8dadc\")\n",
    "ax1.set_title(\"Plot with NetworkX - Risk Score\")\n",
    "nx.draw_kamada_kawai(G, node_size=50, ax=ax1, node_color=color_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "del risk_score[\"Unnamed: 0\"]\n",
    "#Implemento il chord diagram\n",
    "paz_information = risk_score.copy()\n",
    "paz_information[\"Id_paz\"] = range(0, len(paz_information))\n",
    "\n",
    "#Aggiungo una colonna in cui vi è la classe di rischio\n",
    "for i in paz_information.iterrows():\n",
    "    if i[1][0]<=0.03:\n",
    "        paz_information.at[i[0],'Class_Risk'] = \"Basso\"\n",
    "        paz_information.at[i[0],'Color_Risk'] = \"#1d3557\"\n",
    "        \n",
    "    if i[1][0]>0.03 and i[1][0]<=0.3:\n",
    "        paz_information.at[i[0],'Class_Risk'] = \"Medio\"\n",
    "        paz_information.at[i[0],'Color_Risk'] = \"#a8dadc\"\n",
    "    \n",
    "    if i[1][0]>0.3:\n",
    "        paz_information.at[i[0],'Class_Risk'] = \"Alto\"\n",
    "        paz_information.at[i[0],'Color_Risk'] = \"#e63946\"\n",
    "\n",
    "#In questo modo so ogni nodo a quale communities appartiene\n",
    "communities = [\"Comm_\"+ str(x) for x in range(1, len(set(node_color))+1)]\n",
    "color_communities = list(set(node_color))\n",
    "\n",
    "node_communities = []\n",
    "color_node_comm = []\n",
    "for i, k in enumerate(set(node_color)):\n",
    "    for j in node_color:\n",
    "        if j == k:\n",
    "            node_communities.append(communities[i])\n",
    "            color_node_comm.append(j)\n",
    "    \n",
    "#L'obiettivo è associare ad ogni paziente la communities e poi il rischio \n",
    "#Creo un dataframe con il paziente come chiare associato alla communities\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    for j in paz_information.iterrows():\n",
    "        if j[0] in scomplex[\"nodes\"].get(node):\n",
    "            paz_information.at[j[0], 'Communities'] = node_communities[i]\n",
    "            paz_information.at[j[0], 'Color_Comm'] = str(color_node_comm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_file(\"../File Memory/dataframe_Communities.csv\", paz_information ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a715b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungo informazione al dataframe dei pazienti\n",
    "paz_information[\"Età\"]  = eta\n",
    "paz_information[\"Peso\"] = peso\n",
    "paz_information[\"Anni Fumo\"] = anniFumo\n",
    "paz_information[\"HDL Colesterolo\"] = HDLcolesterolo\n",
    "paz_information[\"LDL Colesterolo\"] = LDLcolesterolo\n",
    "paz_information[\"Pressione Sistolica\"]  = presSistolica \n",
    "paz_information[\"Sesso\"] = datasetImputed[\"Sesso\"]\n",
    "paz_information[\"BMI\"] = datasetImputed[\"BMI\"]\n",
    "paz_information['Il paziente soffre di diabete mellito?'] = datasetImputed['Il paziente soffre di diabete mellito?_1']\n",
    "paz_information['Il paziente soffre di ipercolesterolemia?'] = datasetImputed['Il paziente soffre di ipercolesterolemia?_1']\n",
    "paz_information['Il paziente soffre di arteriopatia periferica?'] = datasetImputed['Il paziente soffre di arteriopatia periferica?_1']\n",
    "paz_information['Il paziente soffre di anemia?'] = datasetImputed['Il paziente soffre di anemia?_1']\n",
    "paz_information['Il paziente soffre di insufficienza renale cronica?'] = datasetImputed['Il paziente soffre di insufficienza renale cronica?_1']\n",
    "paz_information['Il paziente soffre di sindrome trombofilica?'] = datasetImputed['Il paziente soffre di sindrome trombofilica?_1']\n",
    "paz_information['Il paziente ha mai avuto sintomi di malattia cardiaca?'] = datasetImputed['Il paziente ha mai avuto sintomi di malattia cardiaca?_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\"Basso\" : '#1d3557',\n",
    "               \"Medio\" : '#a8dadc',\n",
    "               \"Alto\" : '#e63946', \n",
    "               \"Comm_1\": color_communities[0],\n",
    "               \"Comm_2\": color_communities[1],\n",
    "               \"Comm_3\": color_communities[2],\n",
    "               \"Comm_4\": color_communities[3],\n",
    "               \"Comm_5\": color_communities[4],\n",
    "               \"Comm_6\": color_communities[5]}\n",
    "\n",
    "sankey(left = paz_information[\"Class_Risk\"], right = paz_information[\"Communities\"], colorDict= colors_dict, aspect=20, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey(left = paz_information[\"Communities\"], right = paz_information[\"Class_Risk\"], colorDict= colors_dict, aspect=20, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico, all'interno delle varie communities gli istogrammi o box plot delle variabili considerata più informative:\n",
    "var = [\"Peso\", \"Età\", \"Anni Fumo\", \"HDL Colesterolo\", \"LDL Colesterolo\"]\n",
    "color_communities = list(set(node_color))\n",
    "figure = plt.figure(figsize=(27, 24))\n",
    "k = 1\n",
    "for i in var:\n",
    "    for j, comm in enumerate(communities):\n",
    "        ax = plt.subplot(len(set(paz_information[\"Communities\"])), len(var) + 1, k)\n",
    "        x = paz_information.loc[paz_information[\"Communities\"] == comm]\n",
    "        ax.hist(x[i], color = color_communities[j])\n",
    "        ax.set_title(comm + \"_Var=\" + i)\n",
    "        k = k +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_communities = {}\n",
    "for i in communities:\n",
    "    dict_communities.update({i: paz_information.loc[paz_information[\"Communities\"] == i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0893c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comm_selected = [\"Comm_4\",\"Comm_5\",\"Comm_6\"]\n",
    "col= ['Risk_score','Età', 'Peso', 'Anni Fumo', 'HDL Colesterolo',\n",
    "       'LDL Colesterolo', 'Pressione Sistolica', 'Sesso', 'BMI',\n",
    "       'Il paziente soffre di diabete mellito?',\n",
    "       'Il paziente soffre di ipercolesterolemia?',\n",
    "       'Il paziente soffre di arteriopatia periferica?',\n",
    "       'Il paziente soffre di anemia?',\n",
    "       'Il paziente soffre di insufficienza renale cronica?',\n",
    "       'Il paziente soffre di sindrome trombofilica?',\n",
    "       'Il paziente ha mai avuto sintomi di malattia cardiaca?']\n",
    "cat = ['Sesso',\n",
    "       'Il paziente soffre di diabete mellito?',\n",
    "       'Il paziente soffre di ipercolesterolemia?',\n",
    "       'Il paziente soffre di arteriopatia periferica?',\n",
    "       'Il paziente soffre di anemia?',\n",
    "       'Il paziente soffre di insufficienza renale cronica?',\n",
    "       'Il paziente soffre di sindrome trombofilica?',\n",
    "       'Il paziente ha mai avuto sintomi di malattia cardiaca?']\n",
    "groupby = 'Class_Risk'\n",
    "\n",
    "\n",
    "k = 1\n",
    "for i in comm_selected:\n",
    "    x = dict_communities[i]\n",
    "    print(TableOne(data = x, columns=col, categorical=cat,groupby=groupby))\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(5, 5)) \n",
    "plt.hist(dict_communities[\"Comm_4\"][\"Risk_score\"], alpha=0.5, label='Comm 4')\n",
    "plt.hist(dict_communities[\"Comm_5\"][\"Risk_score\"], alpha=0.5, label='Comm 5')\n",
    "plt.hist(dict_communities[\"Comm_6\"][\"Risk_score\"], alpha=0.5, label='Comm 6')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = TableOne(data = x, columns=col, categorical=cat,groupby=groupby)\n",
    "tab.to_latex(\"prova.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
